{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, K, dynamic_plot=False):\n",
    "        self.K = K\n",
    "        self.dynamic_plot = dynamic_plot\n",
    "        self.fig = None\n",
    "        \n",
    "    def reset(self):\n",
    "        self.steps = 0\n",
    "        self.mean = np.random.randn(self.K)\n",
    "        self.optimal_action = np.argmax(self.mean)\n",
    "        self.num_chose_opt = 0\n",
    "        \n",
    "        if self.dynamic_plot:\n",
    "            self.prob_chose_opt = [0]\n",
    "            self.init_plot()\n",
    "        \n",
    "    def init_plot(self):\n",
    "        if not self.fig:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(111)\n",
    "            self.ax.axis(xmin=0, xmax=self.steps+1, ymin=0, ymax=1)\n",
    "            plt.ion()\n",
    "            plt.pause(0.0001)\n",
    "            self.fig.show()\n",
    "            self.fig.canvas.draw()\n",
    "\n",
    "    def update_plot(self):\n",
    "        self.prob_chose_opt.append(self.num_chose_opt / self.steps)\n",
    "        self.ax.clear()\n",
    "        plt.pause(0.0001)\n",
    "        self.ax.axis(xmin=0, xmax=self.steps+1, ymin=0, ymax=1)\n",
    "        self.ax.plot(range(self.steps+1), self.prob_chose_opt, '-r', lw=2)\n",
    "        self.fig.canvas.draw()\n",
    "        \n",
    "    def step(self, action):    \n",
    "        if action >= K:\n",
    "            raise ValueError(\"The action should be less than {0}!\".format(self.K-1))\n",
    "            \n",
    "        self.steps += 1\n",
    "        if action == self.optimal_action:\n",
    "            self.num_chose_opt += 1\n",
    "            \n",
    "        if self.dynamic_plot:\n",
    "            self.update_plot()\n",
    "            \n",
    "        return self.mean[action] + np.random.randn()\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, K, eps):\n",
    "        self.K = K\n",
    "        self.eps = eps\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.Q = np.zeros(K)\n",
    "        self.last_action = None\n",
    "        self.steps = np.zeros(K)\n",
    "            \n",
    "    def get_action(self):\n",
    "        if np.random.random() <= self.eps:\n",
    "            self.last_action = np.random.randint(0, self.K)\n",
    "            self.steps[self.last_action] += 1\n",
    "            return self.last_action\n",
    "        \n",
    "        best_reward = np.where(self.Q == np.amax(self.Q))[0]\n",
    "        self.last_action = np.random.choice(best_reward)\n",
    "        self.steps[self.last_action] += 1\n",
    "        return self.last_action\n",
    "    \n",
    "    def update_policy(self, reward):\n",
    "        self.Q[self.last_action] +=  (1/self.steps[self.last_action]) * (reward - self.Q[self.last_action])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, agent, num_steps):\n",
    "    reward_per_step = np.zeros(num_steps)\n",
    "    num_chose_opt = 0\n",
    "    prob_chose_opt = np.zeros(num_steps)\n",
    "    for i in range(num_steps):\n",
    "        action = agent.get_action()\n",
    "        if action == env.optimal_action:\n",
    "            num_chose_opt += 1\n",
    "        prob_chose_opt[i] = num_chose_opt / (i+1)\n",
    "        reward = env.step(action)\n",
    "        reward_per_step[i] = reward\n",
    "        agent.update_policy(reward)\n",
    "        \n",
    "    return reward_per_step, prob_chose_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "eps = [0.01, 0.1, 0]\n",
    "num_runs = 2000\n",
    "num_steps = 5000\n",
    "average_reward = {}\n",
    "prob_choose_opt = {}\n",
    "\n",
    "for i in range(len(eps)):\n",
    "    key = \"epsilon{0}\".format(eps[i])\n",
    "    average_reward[key] = np.zeros(num_steps)\n",
    "    prob_choose_opt[key] = np.zeros(num_steps)\n",
    "\n",
    "    env = Environment(10)\n",
    "    agent = Agent(K, eps[i])\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        env.reset()\n",
    "        agent.reset()\n",
    "\n",
    "        rew, prob = run(env, agent, num_steps)\n",
    "        average_reward[key] += rew / num_runs\n",
    "        prob_choose_opt[key] += prob / num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,9))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax1.set_xlabel(\"Steps\")\n",
    "ax1.set_ylabel(\"Average Reward\")\n",
    "ax2.set_xlabel(\"Steps\")\n",
    "ax2.set_ylabel(\"Probability of Choosing Optimal Action\")\n",
    "ax2.axis(xmin=0, xmax=num_steps, ymin=0, ymax=1)\n",
    "color = iter(plt.cm.rainbow(np.linspace(0,1,len(eps))))\n",
    "\n",
    "for i in range(len(eps)):\n",
    "    clr = next(color)\n",
    "    ax1.plot(range(num_steps), average_reward[\"epsilon{0}\".format(eps[i])], lw=2, \n",
    "             c=clr, label=r'$\\epsilon = {0}$'.format(eps[i]))\n",
    "    ax2.plot(range(num_steps), prob_choose_opt[\"epsilon{0}\".format(eps[i])], lw=2, \n",
    "             c=clr, label=r'$\\epsilon = {0}$'.format(eps[i]))\n",
    "    \n",
    "ax1.legend(loc=\"lower right\")    \n",
    "ax2.legend(loc=\"upper left\")    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
